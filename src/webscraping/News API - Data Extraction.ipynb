{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d701245-9aff-49fb-8898-7cf8e610c540",
   "metadata": {},
   "source": [
    "## Pulling Articles from News API (Fox News) using following Topics & Issues: \n",
    "\n",
    "##### Politics: Partisan Divide, Foreign Policy, Elections, Immigration Policy, Education Policy\n",
    "##### Environmental: Sustainability, Climate Change, air pollution, recycling, green tech, carbon emission  \n",
    "##### Education: education inequality, remote learning, diversity education, school funding, Student Loan Debt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50690fad-d9b4-4ace-83a0-054513b76ad0",
   "metadata": {},
   "source": [
    "##### Subtopic 1: \n",
    "##### Politics: Partisan Divide, Foreign Policy, Elections, Immigration Policy, Education Policy\n",
    "\n",
    "##### Date Range: March 1, 2025 - March 31, 2025\n",
    "##### Total Articles Pulled: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4311ca9-a012-48f3-b1ef-a4adda73fdc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles for: Partisan Divide\n",
      "Fetching page 1 for query: Partisan Divide...\n",
      "Reached the last page for Partisan Divide. Total articles: 8\n",
      "Saved 8 articles to Output_NewsApi/partisan_divide_articles.json\n",
      "Total articles for 'Partisan Divide': 8\n",
      "Fetching articles for: Foreign Policy\n",
      "Fetching page 1 for query: Foreign Policy...\n",
      "Fetching page 2 for query: Foreign Policy...\n",
      "Fetching page 3 for query: Foreign Policy...\n",
      "Error: 426 Upgrade Required. Please check the API version or protocol.\n",
      "Saved 100 articles to Output_NewsApi/foreign_policy_articles.json\n",
      "Total articles for 'Foreign Policy': 145\n",
      "Fetching articles for: Elections\n",
      "Fetching page 1 for query: Elections...\n",
      "Fetching page 2 for query: Elections...\n",
      "Fetching page 3 for query: Elections...\n",
      "Error: 426 Upgrade Required. Please check the API version or protocol.\n",
      "Saved 100 articles to Output_NewsApi/elections_articles.json\n",
      "Total articles for 'Elections': 118\n",
      "Fetching articles for: Immigration Policy\n",
      "Fetching page 1 for query: Immigration Policy...\n",
      "Fetching page 2 for query: Immigration Policy...\n",
      "Fetching page 3 for query: Immigration Policy...\n",
      "Error: 426 Upgrade Required. Please check the API version or protocol.\n",
      "Saved 100 articles to Output_NewsApi/immigration_policy_articles.json\n",
      "Total articles for 'Immigration Policy': 135\n",
      "Fetching articles for: Education Policy\n",
      "Fetching page 1 for query: Education Policy...\n",
      "Fetching page 2 for query: Education Policy...\n",
      "Fetching page 3 for query: Education Policy...\n",
      "Error: 426 Upgrade Required. Please check the API version or protocol.\n",
      "Saved 100 articles to Output_NewsApi/education_policy_articles.json\n",
      "Total articles for 'Education Policy': 115\n",
      "Total articles for 'Politics' (all subtopics): 521\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Your News API key\n",
    "API_KEY = '726a4b91967a492cb942c4b2ba03b030'\n",
    "\n",
    "# Define the base URL for News API\n",
    "BASE_URL = 'https://newsapi.org/v2/everything'\n",
    "\n",
    "# Calculate the date range: from today to 25 days ago\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')  # Today\n",
    "start_date = (datetime.today() - timedelta(days=25)).strftime('%Y-%m-%d')  # 25 days ago\n",
    "\n",
    "# Set up headers for requests (if needed)\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0',  # Optional: Can help in some cases to mimic a browser request\n",
    "}\n",
    "\n",
    "# Function to fetch articles with exponential backoff\n",
    "def fetch_articles_with_backoff(keyword):\n",
    "    current_page = 1\n",
    "    total_articles = 0\n",
    "    all_articles = []\n",
    "    backoff_time = 1  # Start with 1 second\n",
    "    retry_count = 0  # To track retries for 503 errors\n",
    "\n",
    "    while current_page <= 5:  # Limiting to 5 pages for faster retrieval\n",
    "        print(f\"Fetching page {current_page} for query: {keyword}...\")\n",
    "        params = {\n",
    "            'apiKey': API_KEY,\n",
    "            'q': keyword,  # Keyword to filter articles\n",
    "            'from': start_date,  # Start date (YYYY-MM-DD)\n",
    "            'to': end_date,  # End date (YYYY-MM-DD)\n",
    "            'pageSize': 50,  # Reduced page size to prevent 426 error\n",
    "            'language': 'en',  # Language of the articles\n",
    "            'sortBy': 'publishedAt',  # Sort by the most recent articles\n",
    "            'page': current_page,  # Pagination\n",
    "            'sources': 'fox-news'  # Specific source: Fox News\n",
    "        }\n",
    "\n",
    "        response = requests.get(BASE_URL, params=params, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            articles = data.get('articles', [])\n",
    "            all_articles.extend(articles)\n",
    "            total_articles = data.get('totalResults', 0)\n",
    "\n",
    "            if len(articles) < 50:\n",
    "                print(f\"Reached the last page for {keyword}. Total articles: {total_articles}\")\n",
    "                break\n",
    "\n",
    "            current_page += 1\n",
    "            backoff_time = 1  # Reset backoff time after a successful request\n",
    "            retry_count = 0  # Reset retry count after a successful request\n",
    "\n",
    "        elif response.status_code == 503:\n",
    "            print(f\"Service unavailable (503). Retrying in {backoff_time} seconds...\")\n",
    "            time.sleep(backoff_time)\n",
    "            backoff_time *= 2  # Exponentially increase the backoff time\n",
    "            retry_count += 1\n",
    "\n",
    "            if retry_count >= 3:  # Retry a maximum of 3 times\n",
    "                print(f\"Max retries reached for {keyword}. Exiting.\")\n",
    "                break\n",
    "        elif response.status_code == 426:\n",
    "            print(f\"Error: 426 Upgrade Required. Please check the API version or protocol.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}. Exiting.\")\n",
    "            break\n",
    "\n",
    "    return all_articles, total_articles\n",
    "\n",
    "# Function to save articles to a JSON file\n",
    "def save_articles_to_json(topic, articles):\n",
    "    output_dir = 'Output_NewsApi'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, f'{topic.replace(\" \", \"_\").lower()}_articles.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(articles, file, ensure_ascii=False, indent=4)\n",
    "    print(f\"Saved {len(articles)} articles to {output_file}\")\n",
    "\n",
    "# Subtopics for Politics\n",
    "politics_subtopics = [\n",
    "    \"Partisan Divide\", \"Foreign Policy\", \"Elections\", \"Immigration Policy\", \"Education Policy\"\n",
    "]\n",
    "\n",
    "# Total articles counter for the overarching topic\n",
    "total_politics_articles = 0\n",
    "\n",
    "# Loop through each subtopic\n",
    "for subtopic in politics_subtopics:\n",
    "    print(f\"Fetching articles for: {subtopic}\")\n",
    "    all_articles, total_articles = fetch_articles_with_backoff(subtopic)\n",
    "    save_articles_to_json(subtopic, all_articles)\n",
    "    total_politics_articles += total_articles  # Accumulate total articles for Politics\n",
    "    print(f\"Total articles for '{subtopic}': {total_articles}\")\n",
    "\n",
    "# Print total articles for the Politics topic\n",
    "print(f\"Total articles for 'Politics' (all subtopics): {total_politics_articles}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a5c95-3f64-4c70-9f04-133e9ccfc342",
   "metadata": {},
   "source": [
    "##### Subtopic 2: \n",
    "##### Environmental: Sustainability, Climate Change, air pollution, recycling, green tech, carbon emission\n",
    "\n",
    "##### Date Range: March 1, 2025 - March 31, 2025\n",
    "##### Total Articles Pulled: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82efb507-9e80-486f-ba94-a83baff882a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles for: Sustainability\n",
      "Fetching page 1 for query: Sustainability...\n",
      "Reached the last page for Sustainability. Total articles: 10\n",
      "Saved 10 articles to Output_NewsApi/sustainability_articles.json\n",
      "Total articles for 'Sustainability': 10\n",
      "Fetching articles for: Climate Change\n",
      "Fetching page 1 for query: Climate Change...\n",
      "Reached the last page for Climate Change. Total articles: 46\n",
      "Saved 46 articles to Output_NewsApi/climate_change_articles.json\n",
      "Total articles for 'Climate Change': 46\n",
      "Fetching articles for: Air Pollution\n",
      "Fetching page 1 for query: Air Pollution...\n",
      "Reached the last page for Air Pollution. Total articles: 8\n",
      "Saved 8 articles to Output_NewsApi/air_pollution_articles.json\n",
      "Total articles for 'Air Pollution': 8\n",
      "Fetching articles for: Recycling\n",
      "Fetching page 1 for query: Recycling...\n",
      "Reached the last page for Recycling. Total articles: 2\n",
      "Saved 2 articles to Output_NewsApi/recycling_articles.json\n",
      "Total articles for 'Recycling': 2\n",
      "Fetching articles for: Green Tech\n",
      "Fetching page 1 for query: Green Tech...\n",
      "Reached the last page for Green Tech. Total articles: 7\n",
      "Saved 7 articles to Output_NewsApi/green_tech_articles.json\n",
      "Total articles for 'Green Tech': 7\n",
      "Fetching articles for: Carbon Emission\n",
      "Fetching page 1 for query: Carbon Emission...\n",
      "Reached the last page for Carbon Emission. Total articles: 0\n",
      "Saved 0 articles to Output_NewsApi/carbon_emission_articles.json\n",
      "Total articles for 'Carbon Emission': 0\n",
      "Total articles for 'Environment' (all subtopics): 73\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Your News API key\n",
    "API_KEY = '726a4b91967a492cb942c4b2ba03b030'\n",
    "\n",
    "# Define the base URL for News API\n",
    "BASE_URL = 'https://newsapi.org/v2/everything'\n",
    "\n",
    "# Calculate the date range: from today to 25 days ago\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')  # Today\n",
    "start_date = (datetime.today() - timedelta(days=25)).strftime('%Y-%m-%d')  # 25 days ago\n",
    "\n",
    "# Set up headers for requests (if needed)\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0',  # Optional: Can help in some cases to mimic a browser request\n",
    "}\n",
    "\n",
    "# Function to fetch articles with exponential backoff\n",
    "def fetch_articles_with_backoff(keyword):\n",
    "    current_page = 1\n",
    "    total_articles = 0\n",
    "    all_articles = []\n",
    "    backoff_time = 1  # Start with 1 second\n",
    "    retry_count = 0  # To track retries for 503 errors\n",
    "\n",
    "    while current_page <= 5:  # Limiting to 5 pages for faster retrieval\n",
    "        print(f\"Fetching page {current_page} for query: {keyword}...\")\n",
    "        params = {\n",
    "            'apiKey': API_KEY,\n",
    "            'q': keyword,  # Keyword to filter articles\n",
    "            'from': start_date,  # Start date (YYYY-MM-DD)\n",
    "            'to': end_date,  # End date (YYYY-MM-DD)\n",
    "            'pageSize': 50,  # Reduced page size to prevent 426 error\n",
    "            'language': 'en',  # Language of the articles\n",
    "            'sortBy': 'publishedAt',  # Sort by the most recent articles\n",
    "            'page': current_page,  # Pagination\n",
    "            'sources': 'fox-news'  # Specific source: Fox News\n",
    "        }\n",
    "\n",
    "        response = requests.get(BASE_URL, params=params, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            articles = data.get('articles', [])\n",
    "            all_articles.extend(articles)\n",
    "            total_articles = data.get('totalResults', 0)\n",
    "\n",
    "            if len(articles) < 50:\n",
    "                print(f\"Reached the last page for {keyword}. Total articles: {total_articles}\")\n",
    "                break\n",
    "\n",
    "            current_page += 1\n",
    "            backoff_time = 1  # Reset backoff time after a successful request\n",
    "            retry_count = 0  # Reset retry count after a successful request\n",
    "\n",
    "        elif response.status_code == 503:\n",
    "            print(f\"Service unavailable (503). Retrying in {backoff_time} seconds...\")\n",
    "            time.sleep(backoff_time)\n",
    "            backoff_time *= 2  # Exponentially increase the backoff time\n",
    "            retry_count += 1\n",
    "\n",
    "            if retry_count >= 3:  # Retry a maximum of 3 times\n",
    "                print(f\"Max retries reached for {keyword}. Exiting.\")\n",
    "                break\n",
    "        elif response.status_code == 426:\n",
    "            print(f\"Error: 426 Upgrade Required. Please check the API version or protocol.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}. Exiting.\")\n",
    "            break\n",
    "\n",
    "    return all_articles, total_articles\n",
    "\n",
    "# Function to save articles to a JSON file\n",
    "def save_articles_to_json(topic, articles):\n",
    "    output_dir = 'Output_NewsApi'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, f'{topic.replace(\" \", \"_\").lower()}_articles.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(articles, file, ensure_ascii=False, indent=4)\n",
    "    print(f\"Saved {len(articles)} articles to {output_file}\")\n",
    "\n",
    "# Subtopics for Environment\n",
    "environment_subtopics = [\n",
    "    \"Sustainability\", \"Climate Change\", \"Air Pollution\", \"Recycling\", \"Green Tech\", \"Carbon Emission\"\n",
    "]\n",
    "\n",
    "# Total articles counter for the overarching topic\n",
    "total_environment_articles = 0\n",
    "\n",
    "# Loop through each subtopic\n",
    "for subtopic in environment_subtopics:\n",
    "    print(f\"Fetching articles for: {subtopic}\")\n",
    "    all_articles, total_articles = fetch_articles_with_backoff(subtopic)\n",
    "    save_articles_to_json(subtopic, all_articles)\n",
    "    total_environment_articles += total_articles  # Accumulate total articles for Environment\n",
    "    print(f\"Total articles for '{subtopic}': {total_articles}\")\n",
    "\n",
    "# Print total articles for the Environment topic\n",
    "print(f\"Total articles for 'Environment' (all subtopics): {total_environment_articles}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d6c660-f0ad-47ec-9f2f-ba436a4d26b8",
   "metadata": {},
   "source": [
    "##### Subtopic 3: \n",
    "##### Education: education inequality, remote learning, diversity education, school funding, Student Loan Debt\n",
    "\n",
    "##### Date Range: March 1, 2025 - March 31, 2025\n",
    "##### Total Articles Pulled: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a48f5316-4442-4fb1-9712-0a510b6eab85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles for: Education Inequality\n",
      "Fetching page 1 for query: Education Inequality...\n",
      "Reached the last page for Education Inequality. Total articles: 3\n",
      "Saved 3 articles to Output_NewsApi/education_inequality_articles.json\n",
      "Total articles for 'Education Inequality': 3\n",
      "Fetching articles for: Remote Learning\n",
      "Fetching page 1 for query: Remote Learning...\n",
      "Reached the last page for Remote Learning. Total articles: 1\n",
      "Saved 1 articles to Output_NewsApi/remote_learning_articles.json\n",
      "Total articles for 'Remote Learning': 1\n",
      "Fetching articles for: Diversity Education\n",
      "Fetching page 1 for query: Diversity Education...\n",
      "Reached the last page for Diversity Education. Total articles: 33\n",
      "Saved 33 articles to Output_NewsApi/diversity_education_articles.json\n",
      "Total articles for 'Diversity Education': 33\n",
      "Fetching articles for: School Funding\n",
      "Fetching page 1 for query: School Funding...\n",
      "Fetching page 2 for query: School Funding...\n",
      "Fetching page 3 for query: School Funding...\n",
      "Error: 426 Upgrade Required. Please check the API version or protocol.\n",
      "Saved 100 articles to Output_NewsApi/school_funding_articles.json\n",
      "Total articles for 'School Funding': 113\n",
      "Fetching articles for: Student Loan Debt\n",
      "Fetching page 1 for query: Student Loan Debt...\n",
      "Reached the last page for Student Loan Debt. Total articles: 1\n",
      "Saved 1 articles to Output_NewsApi/student_loan_debt_articles.json\n",
      "Total articles for 'Student Loan Debt': 1\n",
      "Total articles for 'Education' (all subtopics): 151\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Your News API key\n",
    "API_KEY = '726a4b91967a492cb942c4b2ba03b030'\n",
    "\n",
    "# Define the base URL for News API\n",
    "BASE_URL = 'https://newsapi.org/v2/everything'\n",
    "\n",
    "# Calculate the date range: from today to 25 days ago\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')  # Today\n",
    "start_date = (datetime.today() - timedelta(days=25)).strftime('%Y-%m-%d')  # 25 days ago\n",
    "\n",
    "# Set up headers for requests (if needed)\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0',  # Optional: Can help in some cases to mimic a browser request\n",
    "}\n",
    "\n",
    "# Function to fetch articles with exponential backoff\n",
    "def fetch_articles_with_backoff(keyword):\n",
    "    current_page = 1\n",
    "    total_articles = 0\n",
    "    all_articles = []\n",
    "    backoff_time = 1  # Start with 1 second\n",
    "    retry_count = 0  # To track retries for 503 errors\n",
    "\n",
    "    while current_page <= 5:  # Limiting to 5 pages for faster retrieval\n",
    "        print(f\"Fetching page {current_page} for query: {keyword}...\")\n",
    "        params = {\n",
    "            'apiKey': API_KEY,\n",
    "            'q': keyword,  # Keyword to filter articles\n",
    "            'from': start_date,  # Start date (YYYY-MM-DD)\n",
    "            'to': end_date,  # End date (YYYY-MM-DD)\n",
    "            'pageSize': 50,  # Reduced page size to prevent 426 error\n",
    "            'language': 'en',  # Language of the articles\n",
    "            'sortBy': 'publishedAt',  # Sort by the most recent articles\n",
    "            'page': current_page,  # Pagination\n",
    "            'sources': 'fox-news'  # Specific source: Fox News\n",
    "        }\n",
    "\n",
    "        response = requests.get(BASE_URL, params=params, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            articles = data.get('articles', [])\n",
    "            all_articles.extend(articles)\n",
    "            total_articles = data.get('totalResults', 0)\n",
    "\n",
    "            if len(articles) < 50:\n",
    "                print(f\"Reached the last page for {keyword}. Total articles: {total_articles}\")\n",
    "                break\n",
    "\n",
    "            current_page += 1\n",
    "            backoff_time = 1  # Reset backoff time after a successful request\n",
    "            retry_count = 0  # Reset retry count after a successful request\n",
    "\n",
    "        elif response.status_code == 503:\n",
    "            print(f\"Service unavailable (503). Retrying in {backoff_time} seconds...\")\n",
    "            time.sleep(backoff_time)\n",
    "            backoff_time *= 2  # Exponentially increase the backoff time\n",
    "            retry_count += 1\n",
    "\n",
    "            if retry_count >= 3:  # Retry a maximum of 3 times\n",
    "                print(f\"Max retries reached for {keyword}. Exiting.\")\n",
    "                break\n",
    "        elif response.status_code == 426:\n",
    "            print(f\"Error: 426 Upgrade Required. Please check the API version or protocol.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}. Exiting.\")\n",
    "            break\n",
    "\n",
    "    return all_articles, total_articles\n",
    "\n",
    "# Function to save articles to a JSON file\n",
    "def save_articles_to_json(topic, articles):\n",
    "    output_dir = 'Output_NewsApi'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, f'{topic.replace(\" \", \"_\").lower()}_articles.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(articles, file, ensure_ascii=False, indent=4)\n",
    "    print(f\"Saved {len(articles)} articles to {output_file}\")\n",
    "\n",
    "# Subtopics for Education\n",
    "education_subtopics = [\n",
    "    \"Education Inequality\", \"Remote Learning\", \"Diversity Education\", \"School Funding\", \"Student Loan Debt\"\n",
    "]\n",
    "\n",
    "# Total articles counter for the overarching topic\n",
    "total_education_articles = 0\n",
    "\n",
    "# Loop through each subtopic\n",
    "for subtopic in education_subtopics:\n",
    "    print(f\"Fetching articles for: {subtopic}\")\n",
    "    all_articles, total_articles = fetch_articles_with_backoff(subtopic)\n",
    "    save_articles_to_json(subtopic, all_articles)\n",
    "    total_education_articles += total_articles  # Accumulate total articles for Education\n",
    "    print(f\"Total articles for '{subtopic}': {total_articles}\")\n",
    "\n",
    "# Print total articles for the Education topic\n",
    "print(f\"Total articles for 'Education' (all subtopics): {total_education_articles}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5284635-8c21-4f1a-8acb-ffaa624f5961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
